# src/news_collector.py

import json
import logging
from datetime import datetime, timedelta, timezone
from pathlib import Path
import time
import re
import os
from typing import Dict, List, Tuple, Optional
import difflib

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv, find_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê³µí†µ ìœ í‹¸ (KST ë¡œê¹…/ê²½ë¡œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from utils import (
    setup_logging,
    OUTPUT_DIR,
    find_latest_file,
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë¡œê¹… ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
setup_logging()
logger = logging.getLogger("news_collector")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ .env ë¡œë”© (ê³ ì • ê²½ë¡œ + í´ë°±) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_env_with_fallback() -> str:
    """
    /app/config/.env ìš°ì„  â†’ íŒŒì¼ ê¸°ì¤€ í›„ë³´ â†’ CWD í›„ë³´ â†’ find_dotenv ìˆœìœ¼ë¡œ íƒìƒ‰.
    ë¡œë“œ ì„±ê³µ ì‹œ ê²½ë¡œ ë¬¸ìì—´ì„ ë°˜í™˜, ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜.
    """
    candidates = [
        Path("/app/config/.env"),                                    # ì ˆëŒ€ ê²½ë¡œ ìš°ì„ 
        Path(__file__).resolve().parents[1] / "config" / ".env",     # .../src â†’ /app/config/.env
        Path(__file__).resolve().parent / "config" / ".env",         # í˜„ì¬ í´ë” í•˜ìœ„ config/.env
        Path(__file__).resolve().parent / ".env",                    # í˜„ì¬ í´ë” .env
        Path.cwd() / "config" / ".env",                              # CWD/config/.env
        Path.cwd() / ".env",                                         # CWD/.env
    ]

    loaded = ""
    for p in candidates:
        try:
            if p.is_file():
                if load_dotenv(dotenv_path=p, override=False):
                    loaded = str(p)
                    break
        except Exception:
            continue

    if not loaded:
        try:
            found = find_dotenv(usecwd=True)
            if found:
                load_dotenv(found, override=False)
                loaded = found
        except Exception:
            pass

    logger.info(f".env loaded from: {loaded if loaded else 'None'}")
    return loaded

# .env ë¡œë“œ ë° í‚¤ ì½ê¸°
_ = load_env_with_fallback()
NAVER_ID = os.getenv("NAVER_CLIENT_ID", "").strip()
NAVER_SECRET = os.getenv("NAVER_CLIENT_SECRET", "").strip()
DEDUPE_THRESHOLD = float(os.getenv("NEWS_TITLE_SIM_THRESHOLD", "0.85"))

if not (NAVER_ID and NAVER_SECRET):
    logger.warning("NAVER API í‚¤ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤. (/app/config/.env í™•ì¸) ì˜ˆ: NAVER_CLIENT_ID=xxx, NAVER_CLIENT_SECRET=yyy")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìœ í‹¸ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _clean_text(text: str) -> str:
    text = re.sub(r"<.*?>", " ", text)
    text = text.replace("&quot;", '"').replace("&apos;", "'").replace("&amp;", "&")
    return " ".join(text.split())

def _normalize_title(text: str) -> str:
    """ì œëª© ì •ê·œí™”: íƒœê·¸/ì—”í‹°í‹° ì œê±°, ê´„í˜¸ ë‚´ìš© ì œê±°, ì ‘ë¯¸ ë§¤ì²´ëª… ì œê±°, ê³µë°± ì •ë¦¬."""
    if not text:
        return ""
    t = _clean_text(text)
    t = re.sub(r"[\(\[\{ï¼ˆ\[ï½›].*?[\)\]\}ï¼‰\]ï½]", " ", t)  # ê´„í˜¸ë¥˜ ë‚´ìš© ì œê±°
    t = re.sub(r"\s*[-â€“â€”]\s*[^-â€“â€”]{0,20}$", " ", t)       # ëì˜ ' - ë§¤ì²´ëª…' ì œê±° ì‹œë„
    t = " ".join(t.split())
    return t

def _title_similarity(a: str, b: str) -> float:
    return difflib.SequenceMatcher(None, a, b).ratio()

def _dedupe_items_by_title(items: List[Dict], threshold: float = DEDUPE_THRESHOLD) -> List[Dict]:
    """ì œëª© ìœ ì‚¬ë„ë¡œ ì¤‘ë³µ ê¸°ì‚¬ ì œê±°(ì„ ì… ìš°ì„ ). threshold ì´ìƒì´ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼."""
    selected: List[Dict] = []
    norm_titles: List[str] = []
    for it in items:
        title = _normalize_title(it.get("title", "") or "")
        if not title:
            selected.append(it)
            norm_titles.append(title)
            continue
        dup = any(prev and _title_similarity(title, prev) >= threshold for prev in norm_titles)
        if not dup:
            selected.append(it)
            norm_titles.append(title)
    return selected

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ NAVER API / ìŠ¤í¬ë ˆì´í•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _fetch_naver_news_api(keyword: str, num_articles: int) -> List[Dict]:
    """NAVER í‚¤ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜(ì˜ˆì™¸ X). í˜¸ì¶œë¶€ëŠ” ë¹ˆ ê²°ê³¼ë¥¼ ì ì ˆíˆ ì²˜ë¦¬."""
    if not (NAVER_ID and NAVER_SECRET):
        return []
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {"X-Naver-Client-Id": NAVER_ID, "X-Naver-Client-Secret": NAVER_SECRET}
    params = {"query": keyword, "display": num_articles, "sort": "date"}
    r = requests.get(url, headers=headers, params=params, timeout=10)
    r.raise_for_status()
    data = r.json()
    return data.get("items", []) if isinstance(data, dict) else []

def _scrape_article_content(url: str) -> str:
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        r = requests.get(url, headers=headers, timeout=10)
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        soup = BeautifulSoup(r.text, "html.parser")

        selectors = [
            "#articleBodyContents",
            "div.news_end",
            "#articletxt",
            ".article_body",
            "[itemprop='articleBody']",
            "#article-view-content-div",
            "article",
        ]
        container = next((soup.select_one(s) for s in selectors if soup.select_one(s)), None)
        if not container:
            return _clean_text(soup.get_text())

        for unwanted_tag in container.select("script, style, .ad, .aside, .related-news, figure, .promotion"):
            unwanted_tag.decompose()

        return _clean_text(container.get_text())
    except Exception as e:
        logger.warning(f"ë³¸ë¬¸ ìŠ¤í¬ë ˆì´í•‘ ì‹¤íŒ¨({url}): {e}")
        return "ë³¸ë¬¸ì„ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

def _parse_pubdate(pub: str) -> datetime:
    # ì˜ˆ: 'Fri, 30 Aug 2025 12:34:56 +0900'
    dt = datetime.strptime(pub, "%a, %d %b %Y %H:%M:%S %z")
    return dt.astimezone(timezone.utc)

def _normalize_stock_dict(d: Dict) -> Dict:
    """screener ê²°ê³¼ í‚¤ ì •ê·œí™”: Ticker/Name ë³´ì •"""
    out = dict(d)
    if not out.get("Ticker"):
        if out.get("Code"):
            out["Ticker"] = str(out["Code"]).zfill(6)
        elif out.get("ticker"):
            out["Ticker"] = str(out["ticker"]).zfill(6)
    if not out.get("Name"):
        for k in ["Name", "ì¢…ëª©ëª…", "name"]:
            if d.get(k):
                out["Name"] = str(d[k])
                break
    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì½”ì–´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _fetch_news_for_single_stock(
    stock: Dict, cutoff_utc: datetime, num_articles: int
) -> Tuple[str, str]:
    stock = _normalize_stock_dict(stock)
    name, ticker = stock.get("Name"), stock.get("Ticker")
    if not (name and ticker):
        return (str(ticker) if ticker else ""), "ì¢…ëª© ì •ë³´ ëˆ„ë½"

    try:
        # 1) API í˜¸ì¶œ
        items = _fetch_naver_news_api(f'"{name}"', 100)
        if not items:
            return str(ticker), "ë‰´ìŠ¤ API í˜¸ì¶œ ê²°ê³¼ ì—†ìŒ"

        # 2) ê¸°ê°„ í•„í„°
        recent: List[Dict] = []
        for it in items:
            try:
                pub = _parse_pubdate(it["pubDate"])
                if pub >= cutoff_utc:
                    recent.append(it)
            except Exception:
                continue
        if not recent:
            return str(ticker), "ìµœê·¼ ê¸°ê°„ ë‚´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."

        # 3) ì¤‘ë³µ ì œê±° (ì œëª© ìœ ì‚¬ë„)
        recent = _dedupe_items_by_title(recent, threshold=DEDUPE_THRESHOLD)

        # 4) ìŠ¤í¬ë˜í•‘ ë° í¬ë§·
        articles = []
        for it in recent[:num_articles]:
            raw_link = it.get("link") or ""
            link = raw_link if "naver.com" in raw_link else it.get("originallink") or raw_link
            title = _clean_text((it.get("title") or "").strip())
            desc = it.get("description")

            content = _scrape_article_content(link) if link else "ì›ë¬¸ ë§í¬ ì—†ìŒ"
            parts = [f"ì œëª©: {title}"]
            if desc:
                parts.append(f"ìš”ì•½: {_clean_text(desc)}")
            parts.extend([f"ë§í¬: {link or 'N/A'}", f"ë³¸ë¬¸: {content}"])
            articles.append("\n".join(parts))

        return str(ticker), "\n\n---\n\n".join(articles)

    except Exception as e:
        logger.error(f"'{name}'({ticker}) ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        return str(ticker), "ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

def fetch_news_for_stocks(
    stocks: List[Dict],
    num_articles_per_stock: int = 5,
    days: int = 90,
    max_workers: Optional[int] = None,
) -> Dict[str, str]:
    if not stocks:
        return {}

    cutoff_utc = datetime.now(timezone.utc) - timedelta(days=days)
    news_cache: Dict[str, str] = {}
    if max_workers is None:
        max_workers = max(1, min(10, len(stocks)))

    logger.info(f"ğŸ“° {len(stocks)}ê°œ ì¢…ëª© ë‰´ìŠ¤ ë³‘ë ¬ ìˆ˜ì§‘ ì‹œì‘... (ìµœê·¼ {days}ì¼)")
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futures = {
            ex.submit(_fetch_news_for_single_stock, stock, cutoff_utc, num_articles_per_stock): str(_normalize_stock_dict(stock).get("Name", ""))
            for stock in stocks
        }
        for fut in as_completed(futures):
            stock_name = futures[fut]
            try:
                ticker, text = fut.result()
                if ticker:
                    news_cache[ticker] = text
            except Exception as e:
                logger.error(f"'{stock_name}' ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸: {e}", exc_info=True)

    logger.info(f"âœ… {len(news_cache)}/{len(stocks)}ê°œ ì¢…ëª© ë‰´ìŠ¤ ì™„ë£Œ")
    return news_cache

def run_news_collection_from_results_file(
    results_file: Path, num_articles_per_stock: int = 5, days: int = 90
) -> None:
    t0 = time.perf_counter()

    if not results_file.exists():
        logger.error(f"ê²°ê³¼ íŒŒì¼({results_file})ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    stem = results_file.stem  # e.g., "screener_results_YYYYMMDD_KOSPI"
    parts = stem.split("_")
    if len(parts) >= 4:
        fixed_date, market = parts[-2], parts[-1]
    else:
        fixed_date, market = "unknown", "UNKNOWN"

    logger.info(f"ê²°ê³¼ íŒŒì¼ ë¡œë“œ â†’ {results_file}")
    try:
        with open(results_file, "r", encoding="utf-8") as f:
            screened_stocks = json.load(f)
    except (json.JSONDecodeError, IOError) as e:
        logger.error(f"ê²°ê³¼ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
        return

    if not isinstance(screened_stocks, list) or not screened_stocks:
        logger.info("ì¢…ëª©ì´ ì—†ì–´ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¢…ë£Œ.")
        return

    stocks_for_news: List[Dict] = []
    for s in screened_stocks:
        norm = _normalize_stock_dict(s)
        if norm.get("Ticker") and norm.get("Name"):
            stocks_for_news.append({"Name": norm["Name"], "Ticker": str(norm["Ticker"]).zfill(6)})

    if not stocks_for_news:
        logger.warning("Ticker/Nameì´ ìœ íš¨í•œ ì¢…ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    news_data = fetch_news_for_stocks(
        stocks_for_news, num_articles_per_stock=num_articles_per_stock, days=days
    )
    if not news_data:
        logger.warning("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì—†ìŒ.")
        return

    out_file = results_file.parent / f"collected_news_{fixed_date}_{market}.json"
    logger.info(f"ì €ì¥ â†’ {out_file}")
    with open(out_file, "w", encoding="utf-8") as f:
        json.dump(news_data, f, ensure_ascii=False, indent=2)

    logger.info(f"ì™„ë£Œ (ì†Œìš” {time.perf_counter() - t0:.2f}ì´ˆ)")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CLI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="News Collector (compat with screener outputs)")
    parser.add_argument("--file", type=str, help="ìŠ¤í¬ë¦¬ë„ˆ ê²°ê³¼ JSON ê²½ë¡œ")
    parser.add_argument("--articles", type=int, default=5, help="ì¢…ëª©ë³„ ê¸°ì‚¬ ìˆ˜ (ê¸°ë³¸ 5)")
    parser.add_argument("--days", type=int, default=90, help="ìµœê·¼ Nì¼ë§Œ ìˆ˜ì§‘ (ê¸°ë³¸ 90)")
    args = parser.parse_args()

    if args.file:
        run_news_collection_from_results_file(
            Path(args.file), num_articles_per_stock=args.articles, days=args.days
        )
    else:
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        latest = find_latest_file("screener_results_*.json")
        if latest is None:
            logger.error("output/ í´ë”ì— screener_results_*.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            logger.info(f"ìë™ ì„ íƒ: {latest.name}")
            run_news_collection_from_results_file(
                latest, num_articles_per_stock=args.articles, days=args.days
            )
